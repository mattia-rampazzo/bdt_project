{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully:\n",
      "{'latitude': 46.0679, 'longitude': 11.1211, 'pollen_types': ['Alder pollen (grains/m³)', 'Birch pollen (grains/m³)', 'Grass pollen (grains/m³)', 'Mugwort pollen (grains/m³)', 'Olive pollen (grains/m³)', 'Ragweed pollen (grains/m³)'], 'selected_pollen': 'Birch pollen (grains/m³)'}\n",
      "Daily data saved to daily_data.csv\n"
     ]
    }
   ],
   "source": [
    "import openmeteo_requests\n",
    "import requests_cache\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from retry_requests import retry\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# Load configuration from a JSON file hosted on GitHub\n",
    "def load_config_from_github(raw_url):\n",
    "    \"\"\"\n",
    "    Load a JSON configuration file from a GitHub raw URL.\n",
    "\n",
    "    :param raw_url: The raw URL of the JSON file on GitHub\n",
    "    :return: Parsed JSON data\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(raw_url)\n",
    "        response.raise_for_status()  # Raise an HTTPError for bad responses\n",
    "        return response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching the configuration file: {e}\")\n",
    "        return None\n",
    "\n",
    "# GitHub raw link for config.json\n",
    "config_raw_url = \"https://raw.githubusercontent.com/mattia-rampazzo/bdt_project/4724beda4def84536931119d552e1f772f8801de/Predictions/config.json\"\n",
    "\n",
    "# Load latitude and longitude from the config.json hosted on GitHub\n",
    "config = load_config_from_github(config_raw_url)\n",
    "\n",
    "# Debugging: Print the config to verify\n",
    "if config:\n",
    "    print(\"Configuration loaded successfully:\")\n",
    "    print(config)\n",
    "else:\n",
    "    print(\"Failed to load configuration.\")\n",
    "\n",
    "latitude = config[\"latitude\"]\n",
    "longitude = config[\"longitude\"]\n",
    "\n",
    "# Calculate dynamic dates\n",
    "end_date = (datetime.today() - timedelta(days=1)).date()  # Today - 1 day\n",
    "start_date = end_date - timedelta(days=60)  # End date - 2 months\n",
    "\n",
    "# Setup the Open-Meteo API client with cache and retry on error\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after=3600)\n",
    "retry_session = retry(cache_session, retries=5, backoff_factor=0.2)\n",
    "openmeteo = openmeteo_requests.Client(session=retry_session)\n",
    "\n",
    "# Fetch pollen data\n",
    "pollen_params = {\n",
    "    \"latitude\": latitude,\n",
    "    \"longitude\": longitude,\n",
    "    \"hourly\": [\n",
    "        \"alder_pollen\", \"birch_pollen\", \"grass_pollen\",\n",
    "        \"mugwort_pollen\", \"olive_pollen\", \"ragweed_pollen\"\n",
    "    ],\n",
    "    \"start_date\": str(start_date),\n",
    "    \"end_date\": str(end_date)\n",
    "}\n",
    "pollen_response = openmeteo.weather_api(\"https://air-quality-api.open-meteo.com/v1/air-quality\", params=pollen_params)[0]\n",
    "pollen_hourly = pollen_response.Hourly()\n",
    "\n",
    "# Build pollen data dictionary\n",
    "pollen_data = {\n",
    "    \"time\": pd.date_range(\n",
    "        start=pd.to_datetime(pollen_hourly.Time(), unit=\"s\", utc=True),\n",
    "        end=pd.to_datetime(pollen_hourly.TimeEnd(), unit=\"s\", utc=True),\n",
    "        freq=pd.Timedelta(seconds=pollen_hourly.Interval()),\n",
    "        inclusive=\"left\"\n",
    "    )\n",
    "}\n",
    "for i, var in enumerate(pollen_params[\"hourly\"]):\n",
    "    pollen_data[f\"{var} (grains/m³)\"] = pollen_hourly.Variables(i).ValuesAsNumpy()\n",
    "\n",
    "pollen_df = pd.DataFrame(pollen_data)\n",
    "\n",
    "# Fetch weather data\n",
    "weather_params = {\n",
    "    \"latitude\": latitude,\n",
    "    \"longitude\": longitude,\n",
    "    \"start_date\": str(start_date),\n",
    "    \"end_date\": str(end_date),\n",
    "    \"hourly\": [\n",
    "        \"temperature_2m\", \"relative_humidity_2m\", \"precipitation\", \"rain\",\n",
    "        \"cloud_cover\", \"cloud_cover_low\", \"cloud_cover_mid\", \"cloud_cover_high\",\n",
    "        \"wind_speed_10m\", \"soil_temperature_0_to_7cm\"\n",
    "    ]\n",
    "}\n",
    "weather_response = openmeteo.weather_api(\"https://archive-api.open-meteo.com/v1/archive\", params=weather_params)[0]\n",
    "weather_hourly = weather_response.Hourly()\n",
    "\n",
    "# Build weather data dictionary\n",
    "weather_data = {\n",
    "    \"time\": pd.date_range(\n",
    "        start=pd.to_datetime(weather_hourly.Time(), unit=\"s\", utc=True),\n",
    "        end=pd.to_datetime(weather_hourly.TimeEnd(), unit=\"s\", utc=True),\n",
    "        freq=pd.Timedelta(seconds=weather_hourly.Interval()),\n",
    "        inclusive=\"left\"\n",
    "    )\n",
    "}\n",
    "for i, var in enumerate(weather_params[\"hourly\"]):\n",
    "    weather_data[f\"{var}\"] = weather_hourly.Variables(i).ValuesAsNumpy()\n",
    "\n",
    "weather_df = pd.DataFrame(weather_data)\n",
    "\n",
    "# Merge pollen and weather data\n",
    "merged_df = pd.merge(weather_df, pollen_df, on=\"time\", how=\"inner\")\n",
    "\n",
    "# Compute daily averages\n",
    "merged_df[\"date\"] = pd.to_datetime(merged_df[\"time\"]).dt.date\n",
    "daily_data = merged_df.groupby(\"date\").mean().reset_index()\n",
    "\n",
    "# Save only the daily data to a CSV file\n",
    "output_file = \"daily_data.csv\"\n",
    "daily_data.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Daily data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder './ValleysModels1' already exists. Skipping download.\n",
      "Valley boundaries loaded successfully:\n",
      "                    Valley                               Boundary Coordinates  \\\n",
      "0           Alta Valsugana  [(45.908779905847766, 11.199106310628531), (46...   \n",
      "1       Alto Garda e Ledro  [(45.95528476539058, 10.963022214633508), (45....   \n",
      "2              Burgraviato  [(46.50883455076642, 11.131653482240315), (46....   \n",
      "3  Oltradige-Bassa Atesina  [(46.467482496744346, 11.255986244457276), (46...   \n",
      "4            Salto-Sciliar  [(46.51848007594064, 11.232025774529577), (46....   \n",
      "\n",
      "                                            geometry  \n",
      "0  POLYGON ((45.90878 11.19911, 46.10109 11.18055...  \n",
      "1  POLYGON ((45.95528 10.96302, 45.85142 10.88187...  \n",
      "2  POLYGON ((46.50883 11.13165, 46.69839 11.04435...  \n",
      "3  POLYGON ((46.46748 11.25599, 46.3595 11.39256,...  \n",
      "4  POLYGON ((46.51848 11.23203, 46.58579 11.26315...  \n",
      "Using model for valley: Valle di Primiero -> ./ValleysModels1/ValleysModels1/Valle di Primiero_models.pkl\n",
      "Today's Prediction: 0.0\n",
      "Tomorrow's Prediction: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ds/lib/python3.9/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeRegressor from version 1.5.2 when using version 1.6.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/ds/lib/python3.9/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator RandomForestRegressor from version 1.5.2 when using version 1.6.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "import joblib\n",
    "import os\n",
    "import json\n",
    "import gdown\n",
    "import zipfile\n",
    "\n",
    "# Mapping of valleys to their precise model filenames\n",
    "valley_to_model_map = {\n",
    "    \"Valli Giudicarie\": \"Valli Giudicarie_models.pkl\",\n",
    "    \"Val Badia\": \"Val Badia_models.pkl\",\n",
    "    \"Alta Valsugana\": \"Alta Valsugana_models.pkl\",\n",
    "    \"Alto Garda e Ledro\": \"Alto Garda e Ledro_models.pkl\",\n",
    "    \"Val d'Adige\": \"Val d'Adige_models.pkl\",\n",
    "    \"Valsugana\": \"Valsugana_models.pkl\",\n",
    "    \"Valle di Primiero\": \"Valle di Primiero_models.pkl\",\n",
    "    \"Val di Non\": \"Val di Non_models.pkl\",\n",
    "    \"Salto-Sciliar\": \"Salto-Sciliar_models.pkl\",\n",
    "    \"Burgraviato\": \"Burgraviato_models.pkl\",\n",
    "    \"Val di Sole\": \"Val di Sole_models.pkl\",\n",
    "    \"Valle di Fassa\": \"Valle di Fassa_models.pkl\",\n",
    "    \"Vallagarina\": \"Vallagarina_models.pkl\",\n",
    "    \"Valle di Fiemme\": \"Valle di Fiemme_models.pkl\",\n",
    "    \"Val Venosta\": \"Val Venosta_models.pkl\",\n",
    "    \"Val Pusteria\": \"Val Pusteria_models.pkl\",\n",
    "    \"Oltradige-Bassa Atesina\": \"Oltradige-Bassa Atesina_models.pkl\",\n",
    "    \"Valle di Cembra\": \"Valle di Cembra_models.pkl\",\n",
    "    \"Valle dei Laghi\": \"Valle dei Laghi_models.pkl\",\n",
    "    \"Val Rendena\": \"Val Rendena_models.pkl\",\n",
    "}\n",
    "\n",
    "# Online sources\n",
    "valley_boundaries_file_url = \"https://raw.githubusercontent.com/mattia-rampazzo/bdt_project/0ffbd5ae549aa248e733ed77e7399d952abea591/Predictions/Valley_Boundaries.csv\"\n",
    "models_drive_file_url = \"https://drive.google.com/file/d/171oVWCZK17jGUVWV3-p9Q3_qt-8fYzpk/view?usp=share_link\"\n",
    "models_output_directory = \"./ValleysModels1\"\n",
    "\n",
    "# Function to download and extract Google Drive files\n",
    "def download_and_extract_google_drive_file(drive_file_url, output_directory):\n",
    "    file_id = drive_file_url.split(\"/d/\")[1].split(\"/\")[0]\n",
    "    download_url = f\"https://drive.google.com/uc?id={file_id}\"\n",
    "    zip_file_name = \"downloaded_file.zip\"\n",
    "    \n",
    "    print(f\"Downloading file from Google Drive: {drive_file_url}\")\n",
    "    gdown.download(download_url, zip_file_name, quiet=False)\n",
    "\n",
    "    if zipfile.is_zipfile(zip_file_name):\n",
    "        print(f\"Extracting {zip_file_name} to {output_directory}\")\n",
    "        with zipfile.ZipFile(zip_file_name, 'r') as zip_ref:\n",
    "            zip_ref.extractall(output_directory)\n",
    "        os.remove(zip_file_name)\n",
    "        print(f\"File extracted successfully to {output_directory}\")\n",
    "    else:\n",
    "        print(f\"Downloaded file is not a zip file. Saved as {zip_file_name}.\")\n",
    "\n",
    "# Ensure models folder exists\n",
    "if not os.path.exists(models_output_directory):\n",
    "    download_and_extract_google_drive_file(models_drive_file_url, models_output_directory)\n",
    "else:\n",
    "    print(f\"The folder '{models_output_directory}' already exists. Skipping download.\")\n",
    "\n",
    "# Function to adjust model paths\n",
    "def adjust_model_path(output_directory, valley_to_model_map):\n",
    "    for root, dirs, files in os.walk(output_directory):\n",
    "        if any(file.endswith(\".pkl\") for file in files):\n",
    "            model_directory = root\n",
    "            break\n",
    "    else:\n",
    "        raise FileNotFoundError(\"No .pkl model files found in the specified directory.\")\n",
    "\n",
    "    updated_model_map = {}\n",
    "    for valley, model_filename in valley_to_model_map.items():\n",
    "        model_path = os.path.join(model_directory, model_filename)\n",
    "        if os.path.exists(model_path):\n",
    "            updated_model_map[valley] = model_path\n",
    "        else:\n",
    "            print(f\"Warning: Model file for '{valley}' not found. Skipping.\")\n",
    "    \n",
    "    return updated_model_map\n",
    "\n",
    "# Correct the models folder structure\n",
    "valley_to_model_map = adjust_model_path(models_output_directory, valley_to_model_map)\n",
    "\n",
    "# Load valley boundaries from GitHub\n",
    "try:\n",
    "    valley_boundaries = pd.read_csv(valley_boundaries_file_url)\n",
    "    valley_boundaries['geometry'] = valley_boundaries['Boundary Coordinates'].apply(\n",
    "        lambda x: Polygon(eval(x)) if pd.notnull(x) else None\n",
    "    )\n",
    "    valley_gdf = gpd.GeoDataFrame(valley_boundaries, geometry='geometry')\n",
    "    print(\"Valley boundaries loaded successfully:\")\n",
    "    print(valley_gdf.head())\n",
    "except Exception as e:\n",
    "    print(f\"Error loading the valley boundaries file: {e}\")\n",
    "\n",
    "# Function to determine the valley or closest valley\n",
    "def get_valley_or_model(lat, lon):\n",
    "    point = Point(lon, lat)\n",
    "    for _, row in valley_gdf.iterrows():\n",
    "        if row['geometry'] and row['geometry'].contains(point):\n",
    "            valley_name = row['Valley']\n",
    "            model_file = valley_to_model_map.get(valley_name, None)\n",
    "            if model_file:\n",
    "                return valley_name, model_file\n",
    "            else:\n",
    "                raise ValueError(f\"No model file mapping found for valley: {valley_name}\")\n",
    "    valley_gdf['distance'] = valley_gdf['geometry'].apply(lambda geom: geom.centroid.distance(point) if geom else float('inf'))\n",
    "    closest_valley = valley_gdf.loc[valley_gdf['distance'].idxmin()]\n",
    "    valley_name = closest_valley['Valley']\n",
    "    model_file = valley_to_model_map.get(valley_name, None)\n",
    "    if model_file:\n",
    "        return valley_name, model_file\n",
    "    else:\n",
    "        raise ValueError(f\"No model file mapping found for valley: {valley_name}\")\n",
    "\n",
    "# Remaining Functions (unchanged)\n",
    "def rename_columns(data):\n",
    "    column_mapping = {\n",
    "        \"alder_pollen (grains/m³)\": \"Alder pollen (grains/m³)\",\n",
    "        \"birch_pollen (grains/m³)\": \"Birch pollen (grains/m³)\",\n",
    "        \"grass_pollen (grains/m³)\": \"Grass pollen (grains/m³)\",\n",
    "        \"mugwort_pollen (grains/m³)\": \"Mugwort pollen (grains/m³)\",\n",
    "        \"olive_pollen (grains/m³)\": \"Olive pollen (grains/m³)\",\n",
    "        \"ragweed_pollen (grains/m³)\": \"Ragweed pollen (grains/m³)\",\n",
    "        \"precipitation\": \"precipitation (mm)\",\n",
    "        \"temperature_2m\": \"temperature_2m (°C)\"\n",
    "    }\n",
    "    return data.rename(columns=column_mapping)\n",
    "\n",
    "def generate_lagged_features(data, pollen_type):\n",
    "    data[f\"{pollen_type}_lag_1\"] = data[pollen_type].shift(1)\n",
    "    data[f\"{pollen_type}_lag_2\"] = data[pollen_type].shift(2)\n",
    "    data[f\"{pollen_type}_lag_3\"] = data[pollen_type].shift(3)\n",
    "    return data\n",
    "\n",
    "def add_missing_features(data, required_features):\n",
    "    for feature in required_features:\n",
    "        if feature not in data.columns:\n",
    "            data[feature] = 0\n",
    "    return data\n",
    "\n",
    "def load_model(model_path):\n",
    "    if os.path.exists(model_path):\n",
    "        return joblib.load(model_path)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Model file not found at: {model_path}\")\n",
    "\n",
    "def predict_pollen(model, data, features):\n",
    "    X = data[features]\n",
    "    return model.predict(X)\n",
    "\n",
    "def main(config_file, historical_data_path):\n",
    "    config = load_config_from_github(config_file)\n",
    "    lat = config[\"latitude\"]\n",
    "    lon = config[\"longitude\"]\n",
    "    pollen_types = config[\"pollen_types\"]\n",
    "    selected_pollen = config[\"selected_pollen\"]\n",
    "\n",
    "    if selected_pollen not in pollen_types:\n",
    "        raise ValueError(f\"Invalid pollen type selected: {selected_pollen}. Valid options: {pollen_types}\")\n",
    "\n",
    "    valley_name, model_file = get_valley_or_model(lat, lon)\n",
    "    print(f\"Using model for valley: {valley_name} -> {model_file}\")\n",
    "\n",
    "    historical_data = pd.read_csv(historical_data_path)\n",
    "    historical_data = rename_columns(historical_data)\n",
    "    historical_data = generate_lagged_features(historical_data, selected_pollen)\n",
    "\n",
    "    model_dict = load_model(model_file)\n",
    "    if selected_pollen not in model_dict:\n",
    "        raise ValueError(f\"No model found for pollen type: {selected_pollen}\")\n",
    "    model = model_dict[selected_pollen]\n",
    "\n",
    "    required_features = model.feature_names_in_\n",
    "    historical_data = add_missing_features(historical_data, required_features)\n",
    "\n",
    "    prepared_data = historical_data.dropna()\n",
    "\n",
    "    today_data = prepared_data.iloc[-1:]\n",
    "    today_prediction = predict_pollen(model, today_data, required_features)\n",
    "\n",
    "    tomorrow_data = today_data.copy()\n",
    "    tomorrow_data[selected_pollen] = today_prediction\n",
    "    tomorrow_data = generate_lagged_features(tomorrow_data, selected_pollen).iloc[-1:]\n",
    "    tomorrow_prediction = predict_pollen(model, tomorrow_data, required_features)\n",
    "\n",
    "    return {\n",
    "        \"today_prediction\": today_prediction[0],\n",
    "        \"tomorrow_prediction\": tomorrow_prediction[0]\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config_file = config_raw_url\n",
    "    historical_data_file = \"daily_data.csv\"\n",
    "\n",
    "    try:\n",
    "        predictions = main(config_file, historical_data_file)\n",
    "        print(f\"Today's Prediction: {predictions['today_prediction']}\")\n",
    "        print(f\"Tomorrow's Prediction: {predictions['tomorrow_prediction']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
