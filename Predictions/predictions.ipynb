{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily data saved to daily_data.csv\n"
     ]
    }
   ],
   "source": [
    "import openmeteo_requests\n",
    "import requests_cache\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from retry_requests import retry\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "# Load configuration from a JSON file\n",
    "def load_config(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Load latitude and longitude from config.json\n",
    "config = load_config(\"config.json\")\n",
    "latitude = config[\"latitude\"]\n",
    "longitude = config[\"longitude\"]\n",
    "\n",
    "# Calculate dynamic dates\n",
    "end_date = (datetime.today() - timedelta(days=1)).date()  # Today - 1 day\n",
    "start_date = end_date - timedelta(days=60)  # End date - 2 months\n",
    "\n",
    "# Setup the Open-Meteo API client with cache and retry on error\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after=3600)\n",
    "retry_session = retry(cache_session, retries=5, backoff_factor=0.2)\n",
    "openmeteo = openmeteo_requests.Client(session=retry_session)\n",
    "\n",
    "# Fetch pollen data\n",
    "pollen_params = {\n",
    "    \"latitude\": latitude,\n",
    "    \"longitude\": longitude,\n",
    "    \"hourly\": [\n",
    "        \"alder_pollen\", \"birch_pollen\", \"grass_pollen\",\n",
    "        \"mugwort_pollen\", \"olive_pollen\", \"ragweed_pollen\"\n",
    "    ],\n",
    "    \"start_date\": str(start_date),\n",
    "    \"end_date\": str(end_date)\n",
    "}\n",
    "pollen_response = openmeteo.weather_api(\"https://air-quality-api.open-meteo.com/v1/air-quality\", params=pollen_params)[0]\n",
    "pollen_hourly = pollen_response.Hourly()\n",
    "\n",
    "# Build pollen data dictionary\n",
    "pollen_data = {\n",
    "    \"time\": pd.date_range(\n",
    "        start=pd.to_datetime(pollen_hourly.Time(), unit=\"s\", utc=True),\n",
    "        end=pd.to_datetime(pollen_hourly.TimeEnd(), unit=\"s\", utc=True),\n",
    "        freq=pd.Timedelta(seconds=pollen_hourly.Interval()),\n",
    "        inclusive=\"left\"\n",
    "    )\n",
    "}\n",
    "for i, var in enumerate(pollen_params[\"hourly\"]):\n",
    "    pollen_data[f\"{var} (grains/m³)\"] = pollen_hourly.Variables(i).ValuesAsNumpy()\n",
    "\n",
    "pollen_df = pd.DataFrame(pollen_data)\n",
    "\n",
    "# Fetch weather data\n",
    "weather_params = {\n",
    "    \"latitude\": latitude,\n",
    "    \"longitude\": longitude,\n",
    "    \"start_date\": str(start_date),\n",
    "    \"end_date\": str(end_date),\n",
    "    \"hourly\": [\n",
    "        \"temperature_2m\", \"relative_humidity_2m\", \"precipitation\", \"rain\",\n",
    "        \"cloud_cover\", \"cloud_cover_low\", \"cloud_cover_mid\", \"cloud_cover_high\",\n",
    "        \"wind_speed_10m\", \"soil_temperature_0_to_7cm\"\n",
    "    ]\n",
    "}\n",
    "weather_response = openmeteo.weather_api(\"https://archive-api.open-meteo.com/v1/archive\", params=weather_params)[0]\n",
    "weather_hourly = weather_response.Hourly()\n",
    "\n",
    "# Build weather data dictionary\n",
    "weather_data = {\n",
    "    \"time\": pd.date_range(\n",
    "        start=pd.to_datetime(weather_hourly.Time(), unit=\"s\", utc=True),\n",
    "        end=pd.to_datetime(weather_hourly.TimeEnd(), unit=\"s\", utc=True),\n",
    "        freq=pd.Timedelta(seconds=weather_hourly.Interval()),\n",
    "        inclusive=\"left\"\n",
    "    )\n",
    "}\n",
    "for i, var in enumerate(weather_params[\"hourly\"]):\n",
    "    weather_data[f\"{var}\"] = weather_hourly.Variables(i).ValuesAsNumpy()\n",
    "\n",
    "weather_df = pd.DataFrame(weather_data)\n",
    "\n",
    "# Merge pollen and weather data\n",
    "merged_df = pd.merge(weather_df, pollen_df, on=\"time\", how=\"inner\")\n",
    "\n",
    "# Compute daily averages\n",
    "merged_df[\"date\"] = pd.to_datetime(merged_df[\"time\"]).dt.date\n",
    "daily_data = merged_df.groupby(\"date\").mean().reset_index()\n",
    "\n",
    "# Save only the daily data to a CSV file\n",
    "output_file = \"daily_data.csv\"\n",
    "daily_data.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Daily data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model for valley: Valle di Primiero -> ValleysModels/Valle di Primiero_models.pkl\n",
      "Today's Prediction: 0.0\n",
      "Tomorrow's Prediction: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ds/lib/python3.9/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeRegressor from version 1.5.2 when using version 1.6.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/ds/lib/python3.9/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator RandomForestRegressor from version 1.5.2 when using version 1.6.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
   import pandas as pd
import geopandas as gpd
from shapely.geometry import Point, Polygon
import joblib
import os
import json

# Mapping of valleys to their precise model filenames
valley_to_model_map = {
    "Valli Giudicarie": "Valli Giudicarie_models.pkl",
    "Val Badia": "Val Badia_models.pkl",
    "Alta Valsugana": "Alta Valsugana_models.pkl",
    "Alto Garda e Ledro": "Alto Garda e Ledro_models.pkl",
    "Val d'Adige": "Val d'Adige_models.pkl",
    "Valsugana": "Valsugana_models.pkl",
    "Valle di Primiero": "Valle di Primiero_models.pkl",
    "Val di Non": "Val di Non_models.pkl",
    "Salto-Sciliar": "Salto-Sciliar_models.pkl",
    "Burgraviato": "Burgraviato_models.pkl",
    "Val di Sole": "Val di Sole_models.pkl",
    "Valle di Fassa": "Valle di Fassa_models.pkl",
    "Vallagarina": "Vallagarina_models.pkl",
    "Valle di Fiemme": "Valle di Fiemme_models.pkl",
    "Val Venosta": "Val Venosta_models.pkl",
    "Val Pusteria": "Val Pusteria_models.pkl",
    "Oltradige-Bassa Atesina": "Oltradige-Bassa Atesina_models.pkl",
    "Valle di Cembra": "Valle di Cembra_models.pkl",
    "Valle dei Laghi": "Valle dei Laghi_models.pkl",
    "Val Rendena": "Val Rendena_models.pkl",
    "Valle di Rendena": "Valle di Rendena_models.pkl",
}

# Load configuration from JSON
def load_config(file_path):
    with open(file_path, "r") as f:
        return json.load(f)

# File paths for valley boundaries and models
valley_boundaries_file = "/Users/davidegiordani/Desktop/BGTFinalCrack/TrentinoAltoAdigeZoneMapping/Final/Predictions/Valley_Boundaries.csv"
models_folder = "/Users/davidegiordani/Desktop/BGTFinalCrack/TrentinoAltoAdigeZoneMapping/Final/Predictions/ValleysModels1"

# Load the valley boundaries
valley_boundaries = pd.read_csv(valley_boundaries_file)
valley_boundaries['geometry'] = valley_boundaries['boundary_coordinates'].apply(
    lambda x: Polygon(eval(x)) if pd.notnull(x) else None
)
valley_gdf = gpd.GeoDataFrame(valley_boundaries, geometry='geometry')

# Updated function to determine the valley or closest valley
def get_valley_or_model(lat, lon):
    point = Point(lon, lat)
    for _, row in valley_gdf.iterrows():
        if row['geometry'] and row['geometry'].contains(point):
            valley_name = row['valley']
            # Use the precise mapping for model filenames
            model_file = valley_to_model_map.get(valley_name, None)
            if model_file:
                model_file_path = os.path.join(models_folder, model_file)
                return valley_name, model_file_path
            else:
                raise ValueError(f"No model file mapping found for valley: {valley_name}")
    valley_gdf['distance'] = valley_gdf['geometry'].apply(lambda geom: geom.centroid.distance(point) if geom else float('inf'))
    closest_valley = valley_gdf.loc[valley_gdf['distance'].idxmin()]
    valley_name = closest_valley['valley']
    model_file = valley_to_model_map.get(valley_name, None)
    if model_file:
        model_file_path = os.path.join(models_folder, model_file)
        return valley_name, model_file_path
    else:
        raise ValueError(f"No model file mapping found for valley: {valley_name}")

# Function to rename columns for consistency
def rename_columns(data):
    column_mapping = {
        "alder_pollen (grains/m³)": "Alder pollen (grains/m³)",
        "birch_pollen (grains/m³)": "Birch pollen (grains/m³)",
        "grass_pollen (grains/m³)": "Grass pollen (grains/m³)",
        "mugwort_pollen (grains/m³)": "Mugwort pollen (grains/m³)",
        "olive_pollen (grains/m³)": "Olive pollen (grains/m³)",
        "ragweed_pollen (grains/m³)": "Ragweed pollen (grains/m³)",
        "precipitation": "precipitation (mm)",
        "temperature_2m": "temperature_2m (°C)"
    }
    return data.rename(columns=column_mapping)

# Function to generate lagged features
def generate_lagged_features(data, pollen_type):
    data[f"{pollen_type}_lag_1"] = data[pollen_type].shift(1)
    data[f"{pollen_type}_lag_2"] = data[pollen_type].shift(2)
    data[f"{pollen_type}_lag_3"] = data[pollen_type].shift(3)
    return data

# Function to add missing features
def add_missing_features(data, required_features):
    for feature in required_features:
        if feature not in data.columns:
            data[feature] = 0  # Default value for missing features
    return data

# Function to load the model
def load_model(model_path):
    if os.path.exists(model_path):
        return joblib.load(model_path)
    else:
        raise FileNotFoundError(f"Model file not found at: {model_path}")

# Function to predict pollen levels
def predict_pollen(model, data, features):
    X = data[features]
    return model.predict(X)

# Main predictive functionality
def main(config_file, historical_data_path):
    # Load configuration
    config = load_config(config_file)
    lat = config["latitude"]
    lon = config["longitude"]
    pollen_types = config["pollen_types"]
    selected_pollen = config["selected_pollen"]

    # Ensure selected pollen type is valid
    if selected_pollen not in pollen_types:
        raise ValueError(f"Invalid pollen type selected: {selected_pollen}. Valid options: {pollen_types}")

    # Determine the valley and model file
    valley_name, model_file = get_valley_or_model(lat, lon)
    print(f"Using model for valley: {valley_name} -> {model_file}")

    # Load historical data
    historical_data = pd.read_csv(historical_data_path)
    historical_data = rename_columns(historical_data)
    historical_data = generate_lagged_features(historical_data, selected_pollen)

    # Load the dictionary of models
    model_dict = load_model(model_file)

    # Select the specific model for the pollen type
    if selected_pollen not in model_dict:
        raise ValueError(f"No model found for pollen type: {selected_pollen}")
    model = model_dict[selected_pollen]

    # Ensure all required features are present
    required_features = model.feature_names_in_
    historical_data = add_missing_features(historical_data, required_features)

    # Preprocess the historical data
    prepared_data = historical_data.dropna()  # Remove rows with NaN after lagging

    # Make predictions for today
    today_data = prepared_data.iloc[-1:]  # Use the last row for today's prediction
    today_prediction = predict_pollen(model, today_data, required_features)

    # Prepare data for tomorrow's prediction
    tomorrow_data = today_data.copy()
    tomorrow_data[selected_pollen] = today_prediction
    tomorrow_data = generate_lagged_features(tomorrow_data, selected_pollen).iloc[-1:]
    tomorrow_prediction = predict_pollen(model, tomorrow_data, required_features)

    # Return predictions
    return {
        "today_prediction": today_prediction[0],
        "tomorrow_prediction": tomorrow_prediction[0]
    }

# FINAL PART: Predict pollen levels for today and tomorrow
if __name__ == "__main__":
    config_file = "config.json"
    historical_data_file = "daily_data.csv"

    try:
        predictions = main(config_file, historical_data_file)
        print(f"Today's Prediction: {predictions['today_prediction']}")
        print(f"Tomorrow's Prediction: {predictions['tomorrow_prediction']}")
    except Exception as e:
        print(f"An error occurred: {e}")
