{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daily data saved to daily_data.csv\n"
     ]
    }
   ],
   "source": [
    "import openmeteo_requests\n",
    "import requests_cache\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from retry_requests import retry\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "\n",
    "# Load configuration from a JSON file\n",
    "def load_config(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Load latitude and longitude from config.json\n",
    "config = load_config(\"config.json\")\n",
    "latitude = config[\"latitude\"]\n",
    "longitude = config[\"longitude\"]\n",
    "\n",
    "# Calculate dynamic dates\n",
    "end_date = (datetime.today() - timedelta(days=1)).date()  # Today - 1 day\n",
    "start_date = end_date - timedelta(days=60)  # End date - 2 months\n",
    "\n",
    "# Setup the Open-Meteo API client with cache and retry on error\n",
    "cache_session = requests_cache.CachedSession('.cache', expire_after=3600)\n",
    "retry_session = retry(cache_session, retries=5, backoff_factor=0.2)\n",
    "openmeteo = openmeteo_requests.Client(session=retry_session)\n",
    "\n",
    "# Fetch pollen data\n",
    "pollen_params = {\n",
    "    \"latitude\": latitude,\n",
    "    \"longitude\": longitude,\n",
    "    \"hourly\": [\n",
    "        \"alder_pollen\", \"birch_pollen\", \"grass_pollen\",\n",
    "        \"mugwort_pollen\", \"olive_pollen\", \"ragweed_pollen\"\n",
    "    ],\n",
    "    \"start_date\": str(start_date),\n",
    "    \"end_date\": str(end_date)\n",
    "}\n",
    "pollen_response = openmeteo.weather_api(\"https://air-quality-api.open-meteo.com/v1/air-quality\", params=pollen_params)[0]\n",
    "pollen_hourly = pollen_response.Hourly()\n",
    "\n",
    "# Build pollen data dictionary\n",
    "pollen_data = {\n",
    "    \"time\": pd.date_range(\n",
    "        start=pd.to_datetime(pollen_hourly.Time(), unit=\"s\", utc=True),\n",
    "        end=pd.to_datetime(pollen_hourly.TimeEnd(), unit=\"s\", utc=True),\n",
    "        freq=pd.Timedelta(seconds=pollen_hourly.Interval()),\n",
    "        inclusive=\"left\"\n",
    "    )\n",
    "}\n",
    "for i, var in enumerate(pollen_params[\"hourly\"]):\n",
    "    pollen_data[f\"{var} (grains/m³)\"] = pollen_hourly.Variables(i).ValuesAsNumpy()\n",
    "\n",
    "pollen_df = pd.DataFrame(pollen_data)\n",
    "\n",
    "# Fetch weather data\n",
    "weather_params = {\n",
    "    \"latitude\": latitude,\n",
    "    \"longitude\": longitude,\n",
    "    \"start_date\": str(start_date),\n",
    "    \"end_date\": str(end_date),\n",
    "    \"hourly\": [\n",
    "        \"temperature_2m\", \"relative_humidity_2m\", \"precipitation\", \"rain\",\n",
    "        \"cloud_cover\", \"cloud_cover_low\", \"cloud_cover_mid\", \"cloud_cover_high\",\n",
    "        \"wind_speed_10m\", \"soil_temperature_0_to_7cm\"\n",
    "    ]\n",
    "}\n",
    "weather_response = openmeteo.weather_api(\"https://archive-api.open-meteo.com/v1/archive\", params=weather_params)[0]\n",
    "weather_hourly = weather_response.Hourly()\n",
    "\n",
    "# Build weather data dictionary\n",
    "weather_data = {\n",
    "    \"time\": pd.date_range(\n",
    "        start=pd.to_datetime(weather_hourly.Time(), unit=\"s\", utc=True),\n",
    "        end=pd.to_datetime(weather_hourly.TimeEnd(), unit=\"s\", utc=True),\n",
    "        freq=pd.Timedelta(seconds=weather_hourly.Interval()),\n",
    "        inclusive=\"left\"\n",
    "    )\n",
    "}\n",
    "for i, var in enumerate(weather_params[\"hourly\"]):\n",
    "    weather_data[f\"{var}\"] = weather_hourly.Variables(i).ValuesAsNumpy()\n",
    "\n",
    "weather_df = pd.DataFrame(weather_data)\n",
    "\n",
    "# Merge pollen and weather data\n",
    "merged_df = pd.merge(weather_df, pollen_df, on=\"time\", how=\"inner\")\n",
    "\n",
    "# Compute daily averages\n",
    "merged_df[\"date\"] = pd.to_datetime(merged_df[\"time\"]).dt.date\n",
    "daily_data = merged_df.groupby(\"date\").mean().reset_index()\n",
    "\n",
    "# Save only the daily data to a CSV file\n",
    "output_file = \"daily_data.csv\"\n",
    "daily_data.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"Daily data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using model for valley: Valle di Primiero -> ValleysModels/Valle di Primiero_models.pkl\n",
      "Today's Prediction: 0.0\n",
      "Tomorrow's Prediction: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ds/lib/python3.9/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeRegressor from version 1.5.2 when using version 1.6.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/envs/ds/lib/python3.9/site-packages/sklearn/base.py:380: InconsistentVersionWarning: Trying to unpickle estimator RandomForestRegressor from version 1.5.2 when using version 1.6.0. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "import joblib\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Load configuration from JSON\n",
    "def load_config(file_path):\n",
    "    with open(file_path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# File paths for valley boundaries and models\n",
    "valley_boundaries_file = \"Valley_Boundaries.csv\"\n",
    "models_folder = \"ValleysModels/\"\n",
    "\n",
    "# Load the valley boundaries\n",
    "valley_boundaries = pd.read_csv(valley_boundaries_file)\n",
    "valley_boundaries['geometry'] = valley_boundaries['Boundary Coordinates'].apply(\n",
    "    lambda x: Polygon(eval(x)) if pd.notnull(x) else None\n",
    ")\n",
    "valley_gdf = gpd.GeoDataFrame(valley_boundaries, geometry='geometry')\n",
    "\n",
    "# Function to determine the valley or closest valley\n",
    "def get_valley_or_model(lat, lon):\n",
    "    point = Point(lon, lat)\n",
    "    for _, row in valley_gdf.iterrows():\n",
    "        if row['geometry'] and row['geometry'].contains(point):\n",
    "            valley_name = row['Valley']\n",
    "            model_file = os.path.join(models_folder, f\"{valley_name}_models.pkl\")\n",
    "            return valley_name, model_file\n",
    "    valley_gdf['distance'] = valley_gdf['geometry'].apply(lambda geom: geom.centroid.distance(point) if geom else float('inf'))\n",
    "    closest_valley = valley_gdf.loc[valley_gdf['distance'].idxmin()]\n",
    "    valley_name = closest_valley['Valley']\n",
    "    model_file = os.path.join(models_folder, f\"{valley_name}_models.pkl\")\n",
    "    return valley_name, model_file\n",
    "\n",
    "# Function to rename columns for consistency\n",
    "def rename_columns(data):\n",
    "    column_mapping = {\n",
    "        \"alder_pollen (grains/m³)\": \"Alder pollen (grains/m³)\",\n",
    "        \"birch_pollen (grains/m³)\": \"Birch pollen (grains/m³)\",\n",
    "        \"grass_pollen (grains/m³)\": \"Grass pollen (grains/m³)\",\n",
    "        \"mugwort_pollen (grains/m³)\": \"Mugwort pollen (grains/m³)\",\n",
    "        \"olive_pollen (grains/m³)\": \"Olive pollen (grains/m³)\",\n",
    "        \"ragweed_pollen (grains/m³)\": \"Ragweed pollen (grains/m³)\",\n",
    "        \"precipitation\": \"precipitation (mm)\",\n",
    "        \"temperature_2m\": \"temperature_2m (°C)\"\n",
    "    }\n",
    "    return data.rename(columns=column_mapping)\n",
    "\n",
    "# Function to generate lagged features\n",
    "def generate_lagged_features(data, pollen_type):\n",
    "    data[f\"{pollen_type}_lag_1\"] = data[pollen_type].shift(1)\n",
    "    data[f\"{pollen_type}_lag_2\"] = data[pollen_type].shift(2)\n",
    "    data[f\"{pollen_type}_lag_3\"] = data[pollen_type].shift(3)\n",
    "    return data\n",
    "\n",
    "# Function to add missing features\n",
    "def add_missing_features(data, required_features):\n",
    "    for feature in required_features:\n",
    "        if feature not in data.columns:\n",
    "            data[feature] = 0  # Default value for missing features\n",
    "    return data\n",
    "\n",
    "# Function to load the model\n",
    "def load_model(model_path):\n",
    "    if os.path.exists(model_path):\n",
    "        return joblib.load(model_path)\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Model file not found at: {model_path}\")\n",
    "\n",
    "# Function to predict pollen levels\n",
    "def predict_pollen(model, data, features):\n",
    "    X = data[features]\n",
    "    return model.predict(X)\n",
    "\n",
    "# Main predictive functionality\n",
    "def main(config_file, historical_data_path):\n",
    "    # Load configuration\n",
    "    config = load_config(config_file)\n",
    "    lat = config[\"latitude\"]\n",
    "    lon = config[\"longitude\"]\n",
    "    pollen_types = config[\"pollen_types\"]\n",
    "    selected_pollen = config[\"selected_pollen\"]\n",
    "\n",
    "    # Ensure selected pollen type is valid\n",
    "    if selected_pollen not in pollen_types:\n",
    "        raise ValueError(f\"Invalid pollen type selected: {selected_pollen}. Valid options: {pollen_types}\")\n",
    "\n",
    "    # Determine the valley and model file\n",
    "    valley_name, model_file = get_valley_or_model(lat, lon)\n",
    "    print(f\"Using model for valley: {valley_name} -> {model_file}\")\n",
    "\n",
    "    # Load historical data\n",
    "    historical_data = pd.read_csv(historical_data_path)\n",
    "    historical_data = rename_columns(historical_data)\n",
    "    historical_data = generate_lagged_features(historical_data, selected_pollen)\n",
    "\n",
    "    # Load the dictionary of models\n",
    "    model_dict = load_model(model_file)\n",
    "\n",
    "    # Select the specific model for the pollen type\n",
    "    if selected_pollen not in model_dict:\n",
    "        raise ValueError(f\"No model found for pollen type: {selected_pollen}\")\n",
    "    model = model_dict[selected_pollen]\n",
    "\n",
    "    # Ensure all required features are present\n",
    "    required_features = model.feature_names_in_\n",
    "    historical_data = add_missing_features(historical_data, required_features)\n",
    "\n",
    "    # Preprocess the historical data\n",
    "    prepared_data = historical_data.dropna()  # Remove rows with NaN after lagging\n",
    "\n",
    "    # Make predictions for today\n",
    "    today_data = prepared_data.iloc[-1:]  # Use the last row for today's prediction\n",
    "    today_prediction = predict_pollen(model, today_data, required_features)\n",
    "\n",
    "    # Prepare data for tomorrow's prediction\n",
    "    tomorrow_data = today_data.copy()\n",
    "    tomorrow_data[selected_pollen] = today_prediction\n",
    "    tomorrow_data = generate_lagged_features(tomorrow_data, selected_pollen).iloc[-1:]\n",
    "    tomorrow_prediction = predict_pollen(model, tomorrow_data, required_features)\n",
    "\n",
    "    # Return predictions\n",
    "    return {\n",
    "        \"today_prediction\": today_prediction[0],\n",
    "        \"tomorrow_prediction\": tomorrow_prediction[0]\n",
    "    }\n",
    "\n",
    "# FINAL PART: Predict pollen levels for today and tomorrow\n",
    "if __name__ == \"__main__\":\n",
    "    config_file = \"config.json\"\n",
    "    historical_data_file = \"daily_data.csv\"\n",
    "\n",
    "    try:\n",
    "        predictions = main(config_file, historical_data_file)\n",
    "        print(f\"Today's Prediction: {predictions['today_prediction']}\")\n",
    "        print(f\"Tomorrow's Prediction: {predictions['tomorrow_prediction']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
